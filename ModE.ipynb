{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPm+zjTduHCUKaTSxggqXFU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KuvinaDesch/MAT421/blob/main/ModE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculus is very important in data science, and many optimization methods involve calculus.\n",
        "\n",
        "\n",
        "**Limits:**\n",
        "\n",
        "The first important concept in calculus is limits.  When you have a function f(x), you can define the limit as follows:\n",
        "\n",
        "The limit of f(x) as x approaches x0 is equal to y0 if for any epsilon > 0, there exists a delta > 0 such that if |x - x0| < delta, then |f(x) - y0| < epsilon.\n",
        "\n",
        "Essentially, the limit exists if you can find a range of values near x0 where all of their outputs are sufficiently close to y0 for however tight the contraint needs to be.  In some cases, the inputs and outputs are not numbers, instead they're vectors.  If that's the case, we use the vector norm, so we have the same thing but if ||x - x0|| < delta, then ||f(x) - y0|| < epsilon.\n",
        "\n",
        "**Continuity:**\n",
        "\n",
        "We have an intuitive sense of what a continuous function is.  It has no sudden jumps.  But the mathematical definition is that a function is continuous if every f(a) is equal to the limit of f(x) as x approaches a.\n",
        "\n",
        "The extreme value theorem states that if a function is continuous within a certain finite domain, then it has a minimum and maximum within that domain.\n",
        "\n",
        "**Derivatives:**\n",
        "\n",
        "We are all familiar with the slope of a line.  The slope of the straight line between 2 points (x1, y1) and (x2, y2) equals (y2-y1)/(x2-x1).  This represents the rate of change.  For non linear functions, it is more tricky.  We can define the rate of change between 2 points, but what about the slope at a single point?  This is the same as the slope of the tangent line, and also the continuous rate of change.  The definition makes use of limits.  We call it the derivative and denote it f'(x):\n",
        "\n",
        "f'(x0) = lim(h -> 0) (f(x0+h) - f(x0))/h\n",
        "\n",
        "Essentially it is the slope between 2 points as those points get arbitrarily close together.  Since this definition uses limits, the derivative can only be taken for continuous functions.  Taking the derivative at every point creates a new function, f'(x).  This function will be continuous as long as the original function does not have any sharp points.  If it does have sharp points, then those are not differentiable and create a discontinuity.\n",
        "\n",
        "Rolle's theorem states that if you have a continuous funtion and f(a) = f(b), then there exists a c in between a and b such that f'(c) = 0\n",
        "\n",
        "The mean value theorem states that if you have a continuous function from a to b, then there exists a c between a and b such that f'(c) = (f(b)-f(a))/(b-a), or equivalently, there is a point whose slope equals the slope between the 2 endpoints.  Rolle's theorem is a special case of this.\n",
        "\n",
        "**Partial Derivatives**\n",
        "\n",
        "When a function has multiple inputs, you can't take a single derivative.  Instead you have to take a partial derivative.  for a 2d function f(x,y) for example, you have 2 different partial derivatives, ∂f/dx, and ∂f/dy. The definition differs from the single variable derivative in that the h in the numerator gets replaced by h*ei, where ei is the unit vector, in whichever direction you're taking the partial derivative (x for ∂f/dx, y for ∂f/dy).  \n",
        "\n",
        "**Jacobian**\n",
        "\n",
        "The Jacobian matrix comes into play when you have an n dimensional input and m dimensional output.  This can also be thought of as multiple inputs and outputs.  Essentially, f1 to fm are the functions for each dimension of the output.  Each of the m columns will constitute a different one of these.  Each row consists of a different partial derivative.  This basically lets you take the derivative of transformations of space.\n",
        "\n",
        "The chain rule from calculus says that the derivative of f(g(x)) is f'(g(x))\\*g'(x).  It has an equivalent with the jacobian matrix.  J_gf(x0) = J_g(f(x0))\\*J_f(x0).  Here we are treating the Jacobian matrix as a function, but that just means matrix vector multiplication.\n",
        "\n",
        "The gradient of a function represents the sum of all its partial derivatives, where you multiply each one by its corresponding unit vector and you end up with a unique vector.  It can also be thought of as a directional derivative.  The Jacobian of the gradient is called the Hessian.\n",
        "\n",
        "Taylor's theorem says that you can approximate any function around some point with polynomials.  And the way you generate the polynomial is to make sure it has the same value, derivative, second derivative, and so on as the original function at that 1 point.\n",
        "\n",
        "**optimization**\n",
        "\n",
        "It is hard to find the global minimum and maximum, so what we can do instead is to look for a local minimum or maximum.  If the function increases in all directions around a certain point, then it is a local minimum.  You can determine whether the function increases around it using partial derivatives.  \n",
        "\n",
        "**Convexity**\n",
        "\n",
        "A function is convex if for any 2 points on the function, the line segment between them does not pass through any other points on the function.  In a convex function, a local minimum is a global minimum.\n",
        "\n",
        "**Gradient Descent**\n",
        "Gradient descent is a simple but powerful way to find local minima.  Essentially, it's an iterative process where you go downhill.  For any given point, you go to the next one that decreases whatever value you're trying to optimize, and then you repeat.  If there's nowhere left to descend, you have found a local minimum.  The steepest descent method is pretty self explanatory.  You basically descend in whichever direction is steepest.  A more complicated method is logistic regression."
      ],
      "metadata": {
        "id": "yKZ6ZyjhaQ_W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eml0Zm-JZy7x"
      },
      "outputs": [],
      "source": []
    }
  ]
}